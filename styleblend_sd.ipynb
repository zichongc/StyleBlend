{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StyleBlend Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from diffusers import DDIMScheduler\n",
    "from omegaconf import OmegaConf\n",
    "from src.pipeline import StyleBlendT2IPipeline\n",
    "\n",
    "device = 'cuda'\n",
    "cfg = OmegaConf.load('./configs/inference_config.yaml')   \n",
    "dataset_root = cfg.sample_dir\n",
    "weights_dir = cfg.weights_dir\n",
    "\n",
    "# TODO: specify the style name\n",
    "style_name = 'style1'\n",
    "\n",
    "pipeline: StyleBlendT2IPipeline = StyleBlendT2IPipeline.from_pretrained(\n",
    "    cfg.pretrained_model_path, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "pipeline.scheduler = DDIMScheduler.from_config(\n",
    "    os.path.join(cfg.pretrained_model_path, 'scheduler/scheduler_config.json'))\n",
    "\n",
    "pipeline.load_styleblend_weights(\n",
    "    te_lora_path=os.path.join(weights_dir, style_name, f'{style_name}_text_encoder_lora.bin'),\n",
    "    unet_lora_path=os.path.join(weights_dir, style_name, f'{style_name}_unet_lora.bin'),\n",
    "    texture_style_embeds_path=os.path.join(weights_dir, style_name, f'{style_name}_texture_style_embeds.bin'),\n",
    "    composition_style_embeds_path=os.path.join(weights_dir, style_name, f'{style_name}_composition_style_embeds.bin'),\n",
    "    placeholder_composition_style=cfg.placeholder_composition_style,\n",
    "    placeholder_texture_style=cfg.placeholder_texture_style,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Some tips for configuring parameters for inference:\n",
    "- The default feature layers to register (`c2t_self_attn_layers_to_register` and `t2c_self_attn_layers_to_register`) generally work well for most style cases. For styles prone to overfitting, we can register more layers for `c2t` while fewer for `t2c`. \n",
    "- Start with using the middle layers for `c2t` and the side layers for `t2c`. There are 16 layers in SD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: specify the text prompt\n",
    "prompt = 'Tower Bridge'\n",
    "resolution = (768, 768)\n",
    "\n",
    "latents = torch.randn([1, 4, resolution[1]//8, resolution[0]//8]).to(device='cuda', dtype=torch.float16)\n",
    "pipeline.unregister_styleblend_modules()\n",
    "pipeline.register_styleblend_modules(\n",
    "    c2t_self_attn_layers_to_register=[4, 5, 6, 7, 8, 9],\n",
    "    t2c_self_attn_layers_to_register=[0, 1, 2, 3, 10, 11, 12, 13, 14, 15],\n",
    "    scale=0.3,\n",
    "    c2t_step_ratio=0.8,\n",
    "    t2c_step_ratio=0.6,\n",
    ")\n",
    "\n",
    "image = pipeline(prompt, num_inference_steps=30, latents=latents, negative_prompt=[''], eta=1., guidance_scale=7.5).images[1]\n",
    "display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch21cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
